I"¤<hr />

<h2 id="paper">Paper</h2>

<p>The preview PDF of this paper has been published in arxiv: <a href="https://arxiv.org/pdf/1909.07018.pdf">Gumbel-softmax Optimization: A Simple General Framework for Combinatorial Optimization Problems on Graphs</a></p>

<h1 id="discrete-optimization-problems">Discrete Optimization Problems</h1>

<p>The current paradigm of fully differentiable supervised learning doesnâ€™t allow us to make discrete choices in our model because sampling processes are undifferentiable. Traditional methods include score function estimator such as REINFORCE (Williams, 1992) to estimate gradients. However, the REINFORCE suffers from high variance and is consequently slow to converge. Recently reparameterization trick such as Gumbel-softmax provides another approach for differentiable sampling. We can benefit from Gumbel-softmax gradient estimators to solve many discrete optimization problems.</p>

<p>As an example, we adopted the simplest variational mean field. The joint distribution of variables can be written as $P(x)=\prod_{i} p\left(x_{i}\right)$. We focused on three optimization problems below:</p>

<ul>
  <li>modularity in community detection</li>
  <li>MIS problem in graph theory</li>
  <li>Sherrington-Kirkpatrick model energy at $T=0$</li>
</ul>

<p>Here we show our preliminary results below.</p>

<p>MIS problem:
|   Graph  | S2V-DQN | GCNs | Gumbel-softmax  |
|:â€”â€”â€“:|:â€”â€”â€”-:|:â€”â€”-:|:â€”â€”:|
|   Cora   |    1381    |   1451  |  1451  |
| Citeseer |    1705    |   1867  |  1802  |
|  PubMed  |    15709   |  15912  |  15861 |</p>

<p>Sherrington-Kirkpatrick model:
|    N   |     EO    |      PT     |   Gumbel-softmax  |
|:â€”â€”:|:â€”â€”â€”-:|:â€”â€”â€”â€”:|:â€”â€”â€”-:|
|  1024  | â€“0.7563(2) | âˆ’0.75646(22) | -0.7551(2) |
|  2048  |      -     | âˆ’0.75889(14) | -0.7571(3) |
| 10,000 |      -     |       -      | -0.7596(1) |</p>

<p>Future work includes:</p>

<ul>
  <li>inducing more complex variational distribution such as Variational autoregressive networks to improve our results</li>
  <li>we want to know under what circumstances Gumbel-softmax gradient estimator works better than other methods and how to improve it</li>
</ul>
:ET